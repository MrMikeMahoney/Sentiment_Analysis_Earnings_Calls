{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a sentiment analysis on GE transcript calls\n",
    "Sentiment Analysis on publically traded companies earnings conference calls , to get an overall sentiment of the earnings calls from GE (General Electric) company executives.\n",
    "\n",
    "The two conference calls are:\n",
    "\n",
    "Title: General Electric Company (GE) Presents At Electrical Products Group Conference (Transcript) \n",
    "Date of document: May 23, 2018 4:42 PM ET \n",
    "\n",
    "Title: General Electric's (GE) CEO John Flannery on Q1 2018 Results - Earnings Call Transcript \n",
    "Date of document: Apr. 20, 2018 3:04 PM ET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Web Scrape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file web scrapes Alpha Vantage for GE Earnings calls.  I picked GE because it has been (or was) a solid\n",
    "company for the last 30+ years. \n",
    "\n",
    "This program scrapes the site using a random User-Agent from a list (300 User-Agents) from Chrome, Firefox and IE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://seekingalpha.com/article/4176676-general-electric-company-ge-presents-electrical-products-group-conference-transcript\n",
      "https://seekingalpha.com/article/4164470-general-electrics-ge-ceo-john-flannery-q1-2018-results-earnings-call-transcript\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "from urllib.request import Request\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "## Base URL >> Seeking Alpha\n",
    "url_base = \"https://seekingalpha.com/\"\n",
    "## Dictonary >> of the rest of the urls >> for transcripts\n",
    "trans_dict = {\"GE_conf\":\"article/4176676-general-electric-company-ge-presents-electrical-products-group-conference-transcript\",\n",
    "             \"GE_earn_Q1_18\":\"article/4164470-general-electrics-ge-ceo-john-flannery-q1-2018-results-earnings-call-transcript\"}\n",
    "\n",
    "url = url_base + list(trans_dict.values())[0]\n",
    "\n",
    "\n",
    "### Function to clean up HTML tags inside body\n",
    "def html_clean(text):\n",
    "    cleaner = re.compile('<[^<]+?>')    \n",
    "    cleaned_text = re.sub(cleaner, \"\", text)\n",
    "    return cleaned_text\n",
    "\n",
    "### Text File writing data to \n",
    "file_name = \"seek_alpha_ws_test.txt\"\n",
    "f=open(file_name, \"w\")\n",
    "#doc_title = \"This is a test to scrape seeking alpha\\n\"\n",
    "doc_title = \"+-+-+-+-+-+-+-+-+-+\\n\"\n",
    "f.write(doc_title)\n",
    "\n",
    "#### Reading in the list of user agents\n",
    "user_agent_list = pd.read_csv(\"user_agents.txt\", delimiter = \"|\")\n",
    "user_agent_list = list(user_agent_list.iloc[:,0])\n",
    "\n",
    "### Used to get articles Articles >> Which would be like containers\n",
    "req = Request(url, headers={\n",
    "    'User-Agent': (random.choice(user_agent_list))})\n",
    "uClient = uReq (req)  # sends GET request to URL\n",
    "page_html = uClient.read ()  # reads returned data and puts it in a variable\n",
    "uClient.close ()  # close the connection\n",
    "page_soup = soup(page_html, \"html.parser\")\n",
    "articles = page_soup.find(\"article\") # For article body\n",
    "\n",
    "\n",
    "#### This loop goes through the different url's in dictonrary\n",
    "for key in range(len(trans_dict.values())):\n",
    "    my_url = url_base + list(trans_dict.values())[key]\n",
    "    print(my_url)\n",
    "    req = Request(my_url, headers={\n",
    "    'User-Agent': (random.choice(user_agent_list))})\n",
    "    uClient = uReq (req)  # sends GET request to URL\n",
    "    page_html = uClient.read ()  # reads returned data and puts it in a variable\n",
    "    uClient.close ()  # close the connection\n",
    "    ### Getting Articles >> Which would be like containers\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "    articles = page_soup.find(\"article\") # For article body\n",
    "    for article in articles:\n",
    "\n",
    "        title_container = articles.find(\"h1\", {\"itemprop\":\"headline\"}) # Title container\n",
    "        title = title_container.text # Extracts title from container\n",
    "    \n",
    "        time_container = articles.find(\"time\", {\"itemprop\":\"datePublished\"}) # Time container\n",
    "        time = time_container.text # Extracts the Date and Time article was published\n",
    "    \n",
    "        body_container = articles.findAll(\"p\")\n",
    "        body = str(body_container) # Converts body_container resultset >> to a string\n",
    "        body = html_clean(body) # Cleaning out the HTML tags\n",
    "        body = body.replace(\"&amp;\", \"&\")\n",
    "        \n",
    "    f.write(\"Title: \\n\"+title+\"\\nDate of document: \\n\"+time+\"\\nContent: \\n\"+body+\"\\n+-+-+-+-+-+-+-+-+-+\\n\")\n",
    "    sleep(7)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Positive: ', 9.88), ('Negative: ', 1.3), ('Neutral: ', 88.82), ('Compound: ', 19.43))\n",
      "(('Positive: ', 10.0), ('Negative: ', 2.1), ('Neutral: ', 86.62), ('Compound: ', 17.16))\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SIA\n",
    "import re\n",
    "\n",
    "\n",
    "analyzer = SIA()\n",
    "\n",
    "#### Reading the whole documnet >> VaderSent reads the whole documnet >> Creates a list that stores sentiment values\n",
    "sent_sentence = []\n",
    "doc_line_count = []\n",
    "#doc_list = []\n",
    "file = \"seek_alpha_ws_test.txt\"\n",
    "with open(file, \"r\") as fileinput:\n",
    "    #sent_sentence = []\n",
    "    for line in fileinput:\n",
    "        #if re.search(\"Content:\", line):\n",
    "        if re.search(\"\\[\", line):\n",
    "            line_count = None\n",
    "            line = line.rstrip() # Strips the white space\n",
    "            split_line = line.split(\".\")\n",
    "            line_count = len(split_line)\n",
    "            doc_line_count.append(line_count)\n",
    "            for sent_line in split_line:\n",
    "                sentiment = analyzer.polarity_scores(sent_line)\n",
    "                sent_sentence.append(sentiment)\n",
    "                #doc_list.append(sent_sentence)\n",
    "\n",
    "\n",
    "#### Getting the docs from sent_sentence >> indexing by doc_line_count\n",
    "ge_doc_1 = sent_sentence[0:446] #General Electric Company (GE) Presents At Electrical Products Group Conference (Transcript)\n",
    "ge_doc_2 = sent_sentence[446:] #General Electric's (GE) CEO John Flannery on Q1 2018 Results - Earnings Call Transcript\n",
    "\n",
    "\n",
    "#### This function is a loop that breaks down the list >> which has a dictonary inside\n",
    "#### It takes the keys and puts each value into a corresponding list\n",
    "\n",
    "###### For ge_doc_2\n",
    "def doc_sent(doc):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    compound = []\n",
    "    i = 0\n",
    "    for i in range(len(doc)):\n",
    "       positive.append(doc[i][\"pos\"])\n",
    "       negative.append(doc[i][\"neg\"])\n",
    "       neutral.append(doc[i][\"neu\"])\n",
    "       compound.append(doc[i][\"compound\"])\n",
    "    \n",
    "    \n",
    "    ### Gets an average for the corresponding sentiment\n",
    "    positive = sum(positive)/len(positive)\n",
    "    negative = sum(negative)/len(negative)\n",
    "    neutral = sum(neutral)/len(neutral)\n",
    "    compound = sum(compound)/len(compound)\n",
    "    \n",
    "    ### Rounds the sentiment and turns it into a percent\n",
    "    positive = round((positive *100),2)\n",
    "    negative = round((negative *100),2)\n",
    "    neutral = round((neutral *100),2)\n",
    "    compound = round((compound *100),2)\n",
    "    positive = \"Positive: \", positive\n",
    "    negative = \"Negative: \", negative\n",
    "    neutral = \"Neutral: \", neutral\n",
    "    compound = \"Compound: \", compound\n",
    "    return positive,negative,neutral,compound\n",
    "\n",
    "## Creating variables that hold the sentiment from each corresponding document from the doc_sent function\n",
    "GE_doc_1_sentiment = doc_sent(ge_doc_1)\n",
    "GE_doc_2_sentiment = doc_sent(ge_doc_2)\n",
    "\n",
    "## Seeing the results for each documents sentiment\n",
    "print(GE_doc_1_sentiment)\n",
    "print(GE_doc_2_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
